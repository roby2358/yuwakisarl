Short version: treat minigam like a tiny, stochastic backgammon. Build a fast rules engine, then climb three rungs: a searchy baseline, a self-play learner, and small exact solvers for endgames. You’ll get strong play fast, plus a path to near-optimal.

Start with the board math in your head. A state is just how 8 checkers per side are distributed over eight bins (bar, points 1–6, borne-off), plus side-to-move and the current dice. That’s small enough to exploit symmetry, but still too big to brute-force immediately. The dice make it an MDP (Markov decision process), so value iteration is in play in principle, but legal-move branching makes a pure DP grindy. That suggests a hybrid: search with a learned value to guide it, and exact tables where the space collapses.

Build a clean environment first. Canonicalize orientation so the current player always “moves right,” which halves your data. Encode state as a compact tensor: two 8-length integer vectors (our bins, their bins), plus flags for side, variant (race vs “all-on-board-before-off”), and dice. You’ll want a legal-move generator that enumerates all plays for the rolled dice, including the “one die even if it kills the other” rule, hits, entries, and bear-offs.

Your first agent shouldn’t be clever; it should be dependable. Write an expectimax with chance nodes for the dice: for a given roll, expand all legal plays to a small depth (one or two plies is already useful on a six-point board). Evaluate leaves with a hand-rolled heuristic: pip-distance to bear-off, number of made points, exposure of blots, entries blocked for the opponent, opponent on bar, and progress already borne off. This gives you a fast, beatable sparring partner and creates labeled trajectories for bootstrapping.

Then switch to self-play with search in the loop. AlphaZero-style, but simplified for dice. Use MCTS with PUCT; treat the dice as chance nodes. Either sample dice in the tree (stochastic MCTS) or expand chance nodes with their probabilities (36 outcomes) but cap width with stratified sampling so you don’t explode. The network can be tiny: an MLP over the 16 integers (two 8-bins), plus a few binary features. Three or four hidden layers of ~256 units is plenty. Train it to predict (a) a scalar value (win probability or expected reward) and (b) a policy over legal plays. Since the action space changes by roll, define the policy on a canonical “move encoding”: for each roll, serialize each legal complete play into a fixed-length tuple (enter/move/bear-off steps with start→end indices), hash to an index for that position only, and train the policy on the normalized visit counts from MCTS. In effect, you learn to imitate your own search (expert iteration) while the value head stabilizes the tree.

Because dice are noisy on a tiny board, add two stabilizers. First, Dirichlet noise at the root so you don’t overfit to early luck. Second, a small per-move time penalty in the reward (+1 win, −1 loss, −ε per turn) to prefer faster wins and discourage meaningless stalling in fortress situations. Keep a league where each new checkpoint plays matches against the last N checkpoints; promote only if it clears a margin. That prevents regressions from lucky training windows.

Meanwhile, carve the endgame with math. Any position where one side has k≤4 checkers left can likely be solved exactly with retrograde analysis under your exact-bear-off rule. Build tablebases keyed by both sides’ residual distributions and the side to move, backing up expected values under the dice. Use those during training and search as perfect leaves. They make the AI ruthless at the finish and speed up learning because the network doesn’t have to approximate trivial endings.

You’ll want a second variant head. The “all-checkers-must-be-on-board before bearing off” toggle really does change optimal play. Learn a single network with a binary variant bit; the search policy adapts, and you don’t split data.

A quick training cadence works well here. Generate self-play with 200–800 simulations per move in MCTS (it’s a tiny tree when the board is six points). Batch games on GPU/TPU; train with Adam, cosine LR decay, standard L2, maybe label-smoothing on policy. In the first few thousand games, mix in a bit of the heuristic expectimax as an opponent to keep the target sharp early. After that, pit new nets against the last “best” with fixed seeds for reproducibility. You’ll converge shockingly fast because the domain is small and richly interactive.

For analysis and explainability, keep two extra tools. One is a pure rollout bot: given a move, run 5–10k random playouts with light bias (always hit a blot, prefer to enter when possible) and report win rates. The other is a feature-based evaluator you can print: pip count difference, open-entry count for the opponent, made-point count, bar pressure. When the neural bot picks a weird line, compare it to these features; you’ll learn new heuristics the model “discovered.”

If you later want “optimal,” you have two routes. One is to shrink the game slightly (fewer checkers, or forbid entering to made points as you already do) and run full value iteration with perfect hashing and symmetry reduction; you might crack it. The other is to keep the current rules and build bigger tablebases from the end backward while using MCTS+NN for the midgame; hybrid solvers like that often get you within noise of perfect.

Concrete next steps: implement the environment with canonical orientation and a rock-solid move generator; code the expectimax baseline and the rollout bot; add the small MLP value net to guide search; then flip on the AlphaZero loop with stochastic MCTS, Dirichlet at the root, league play, and endgame tablebases stitched in. Within a few days of self-play, you’ll have an AI that feels mean, fast, and very hard to bluff—exactly the vibe this little game asks for.